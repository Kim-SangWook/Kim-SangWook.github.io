1. INTRODUCTION AND LITERATURE REVIEW
=====================================

í†µê³„ì  ë°©ë²•ì€ ì¢…ì¢… ë°ì´í„°ê°€ íŠ¹ì •ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ëŠ” ê°€ì •ì„ ê¸°ë°˜ìœ¼ë¡œ
ë„ì¶œëœë‹¤.

ê°€ì •ëœ ë¶„í¬ì— ìƒê´€ì—†ì´ ê²°ë¡ ì˜ íƒ€ë‹¹ì„±ì€ í™•ë¥ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ë°ì´í„°ì— ì˜
í”¼íŒ…ë˜ëŠ”ì§€ì— ë‹¬ë ¤ìˆë‹¤.

ë”°ë¼ì„œ íƒ€ë‹¹í•œ ë¶„ì„ì„ í•˜ë ¤ë©´ ê¸°ë³¸ ë°ì´í„° ëª¨ë¸ì´ ê°€ì •ì„ ì¶©ì¡±í•˜ëŠ”ì§€
ê²€ì •í•˜ëŠ” ì í•©ë„ í…ŒìŠ¤íŠ¸ë¥¼ ê°œë°œí•˜ëŠ”ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ gofëŠ” ëª¨ìˆ˜ì  ëª¨ë¸ vs ë¹„ëª¨ìˆ˜ì  ëª¨ë¸ ì´ë‹¤.

ì´ëŸ¬í•œ í…ŒìŠ¤íŠ¸ë¥¼ ê°œë°œí•  ë•Œ ì§ë©´í•  ê³¼ì œëŠ” ë¹„ëª¨ìˆ˜ì  ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì´ë‹¤.

1.1 History of Goodness-of-Fit Testing
--------------------------------------

gofë¥¼ ë‹¨ë³€ëŸ‰ ë°ì´í„°ì—ì„œì˜ ë‹¤ë³€ëŸ‰ìœ¼ë¡œ í™•ì¥í•œë‹¤.

ë¹ˆë„ì£¼ì˜ì™€ ë² ì´ì§€ì•ˆ ë‘ ê°€ì§€ì— ëŒ€í•´ì„œ ë³´ì—¬ì¤€ë‹¤.

### 1.1.1 Frequentist Tests

### 1.1.2 Comparison of Frequentist and Bayesian Hypothesis Testing

ì í•©ë„ ê²€ì •ì—ëŒ€í•œ ë² ì´ì§€ì•ˆ ì ‘ê·¼ì„ ì„¤ëª…í•˜ê¸° ì „ì— ë¹ˆë„ì£¼ì˜ vs ë² ì´ì§€ì•ˆ
ë¹„êµí•˜ë©´ì„œ ë² ì´ì§€ì•ˆì„ ì„ í˜¸í•˜ëŠ” ì´ìœ ì— ëŒ€í•´ ë¨¼ì € ì„¤ëª…í•œë‹¤.

ë¹ˆë„ì£¼ì˜ì  ì í•©ë„ ê²€ì •ì—ì„œ ê°€ì„¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ë‹¤.
$$
\\begin{array}{l}
H\_{0}: Y\_{1}, \\ldots, Y\_{n} \\sim f(\\cdot \\mid \\theta \\in \\Theta) \\\\
H\_{1}: Y\_{1}, \\ldots, Y\_{n} \\nsim f(\\cdot \\mid \\theta \\in \\Theta)
\\end{array}
$$

p-ê°’ì´ ì‘ì„ ê²½ìš° nullì„ ê¸°ê°í•˜ì§€ë§Œ ëŒ€ì•ˆ ëª¨ë¸ì´ ëª¨í˜¸í•œ ê²½ìš° ëª¨ë¸ì— ëŒ€í•œ
ì •ë³´ëŠ” ê±°ì˜ ì—†ë‹¤.

p-ê°’ì´ í´ ê²½ìš° nullì„ ê¸°ê°í•˜ì§„ ëª»í•˜ì§€ë§Œ nullì´ Trueë¼ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤.
ì¼ë°˜ì ì¸ p=0.05ì„ ì´ìš©í•œ ê²°ë¡ ì€ ì¢…ì¢… nullì— ëŒ€í•´ì„œ ë”ë§ì€ ì¦ê±°ê°€
ìˆì„ìˆ˜ìˆë‹¤.

í°ë°ì´í„°(n -&gt; ë¬´í•œëŒ€ë¡œ ê°„ë‹¤ë©´)ì—ì„œ ìœ ì˜ìˆ˜ì¤€ì´ 0ì´ ë˜ì–´ì•¼ í•œë‹¤ëŠ”
ì˜ˆì‹œë„ ìˆë‹¤.

ë”°ë¼ì„œ ì í•©ë„ê²€ì •ì—ì„œëŠ” ë¹ˆë„ì£¼ì˜ì  ì ‘ê·¼ì´ ë°”ëŒì§í•˜ì§€ ì•Šë‹¤.

ì´ì œ ë² ì´ì§€ì•ˆ ê´€ì ì—ì„œ ì í•©ë„ ê²€ì •ì„ ê³ ë ¤í•˜ì.
$$
\\begin{array}{l}
H\_{0}: Y\_{1}, \\ldots, Y\_{n} \\sim f(\\cdot \\mid \\theta \\in \\Theta) \\\\
H\_{1}: Y\_{1}, \\ldots, Y\_{n} \\nsim f(\\cdot \\mid \\theta \\in \\Theta)
\\end{array}
$$
we compute the Bayes factor in favor of the alternative model
$$
\\mathrm{BF}=\\frac{\\int\\left\[\\prod\_{i=1}^{n} g\\left(Y\_{i} \\mid \\theta\_{1}\\right)\\right\] p\\left(\\theta\_{1}\\right) d \\theta\_{1}}{\\int\\left\[\\prod\_{i=1}^{n} f\\left(Y\_{i} \\mid \\theta\_{0}\\right)\\right\] \\pi\\left(\\theta\_{0}\\right) d \\theta\_{0}},
$$
is the ratio of marginal likelihoods

<figure>
<img src="fig1.jpg" style="width:50.0%" alt="ë² ì´ì¦ˆí™í„° ê°’ì— ë”°ë¥¸ ëŒ€ì•ˆëª¨ë¸ì— ëŒ€í•œ ì¦ê±°" /><figcaption aria-hidden="true">ë² ì´ì¦ˆí™í„° ê°’ì— ë”°ë¥¸ ëŒ€ì•ˆëª¨ë¸ì— ëŒ€í•œ ì¦ê±°</figcaption>
</figure>

ë² ì´ì¦ˆ íŒ©í„°ì˜ ì¤‘ìš”í•œ ì†ì„±ì¤‘ í•˜ë‚˜ì¸ ë² ì´ì¦ˆíŒ©í„° ì¼ê´€ì„±(Bayes factor
consistency)ëŠ” ì •ì˜1ì— ì •ì˜ë˜ì–´ìˆë‹¤.

#### Definition 1. (Bayes Factor Consistency): The Bayes factor defined by BF comparing the alternative model â„±<sub>1</sub> to the null model â„±<sub>0</sub> in Bayesian hypothesis testing is consistent if, as *n*â€„â†’â€„âˆ :

-   *B**F*â€„â†’â€„âˆ(logâ€†(*B**F*)â€„â†’â€„âˆ) when â„±<sub>1</sub> is the true model;
    and
-   *B**F*â€„â†’â€„0(logâ€†(*B**F*)â€„â†’â€„â€…âˆ’â€…âˆ) when â„±<sub>0</sub> is the true
    model.

ì´ëŠ” í‘œë³¸ í¬ê¸°ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ì œ 1,2ì¢… ì˜¤ë¥˜ê°€ ë°œìƒí•  í™•ë¥ ì´ 0ì´ë˜ëŠ”
ê²½í–¥ì´ ìˆìŒì„ ì˜ë¯¸í•œë‹¤.

ë¬´ë¡  ë‘ ëª¨ë¸ì´ ë°ì´í„°ì— ì í•©í•˜ì§€ ì•Šì•„ë„ í•˜ë‚˜ì˜ ëª¨ë¸ì´ ì„ í˜¸ëœë‹¤. í•˜ì§€ë§Œ
ì—¬ê¸°ì„œ ëŒ€ì•ˆëª¨ë¸ì€ Kullback-Leiblerê´€ì ì—ì„œ Trueì— ê°€ê¹ê²Œ ì •ì˜ëœë‹¤.

ì² í•™ì  ê´€ì ì—ì„œ ë¹ˆë„ì£¼ì˜ë³´ë‹¤ ë² ì´ì§€ì•ˆì´ ì í•©ë„ ê²€ì •ì— ì ì ˆí•˜ë‹¤.

ë² ì´ì¦ˆíŒ©í„°ëŠ” ê·€ë¬´ê°€ì„¤,ëŒ€ë¦½ê°€ì„¤ ëª¨ë‘ì— ëŒ€í•œ ì£¼ë³€ê°€ëŠ¥ë„ê°€ í•„ìš”í•˜ë‹¤.

null marginal likelihoodì€ ìœ í•œì°¨ì›ì—ì„œ ì ë¶„, ë°˜ë©´ alternative marginal
likelihood ëŠ” ì¢…ì¢… ë§¤ìš° ë†’ì€ ì°¨ì›ì—ì„œì˜ ì ë¶„ì´ê¸° ë•Œë¬¸ì— ì–´ë µë‹¤.

ë”°ë¼ì„œ MCMCì•Œê³ ë¦¬ì¦˜ì´ í•„ìš”í•˜ì—¬ ì»´í“¨íŒ…ì˜ ì„±ëŠ¥ì´ ë‚®ì€ ì˜ˆì „ì—ëŠ” ë¹ˆë„ì£¼ì˜ê°€
ë” ì„ í˜¸ë˜ì—ˆë‹¤.

### 1.1.3 Bayesian Tests

ë‹¨ë³€ëŸ‰ ë°ì´í„°ì—ì„œì˜ ì´ˆê¸° ë² ì´ì§€ì•ˆ ì í•©ë„ ê²€ì •ì€ ëŒ€ë¶€ë¶„ Dirichlet
processes, Polya tree processes, and Gaussian processesë¥¼ í™œìš©í•˜ëŠ”
ë² ì´ì§€ì•ˆë¹„ëª¨ìˆ˜ ë°€ë„ì¶”ì •ì„ ì´ìš©í•˜ì˜€ë‹¤.

Dirichlet processë¥¼ ì´ìš©í•œ ë² ì´ì§€ì•ˆ ë¹„ëª¨ìˆ˜ ì í•©ë„ ê²€ì •, alternative
modelì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.
$$
\\begin{aligned}
\\mathbf{y} \\mid \\underline{F}, \\underline{\\theta} & \\sim \\prod\_{i=1}^{s} \\prod\_{j=1}^{n\_{i}} F\_{i}\\left(y\_{i j} \\mid \\theta\_{i}\\right), \\quad \\underline{F}=\\left(F\_{1}, \\ldots, F\_{s}\\right) \\in \\mathcal{F} \\quad \\underline{\\theta}=\\left(\\theta\_{1}, \\ldots, \\theta\_{s}\\right) \\in \\Theta \\\\
\\underline{F} \\mid \\underline{\\theta} & \\sim \\prod\_{i=1}^{s} \\mathcal{D}\_{i}, \\quad \\mathcal{D}\_{i} \\sim \\text { Dirichlet process with measure } \\alpha\_{i}\\left(\\theta\_{i}, y\\right) \\\\
\\underline{\\theta} & \\sim \\mathcal{Q}
\\end{aligned}
$$

ğ’¬ is the prior distribution function of *Î¸*. ì´ ë‘ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©´
nullì„ ì„ í˜¸í•˜ëŠ” ë² ì´ì¦ˆíŒ©í„°ë¥¼ ì‰½ê³„ ê³„ì‚°í•  ìˆ˜ ìˆì§€ë§Œ ì—°ì†ì ì¸ null ëª¨ë¸ì„
ê²€ì •í•  ë•Œ ë””ë¦¬í´ë ˆ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë¶€ì ì ˆí•˜ë‹¤.

ë”°ë¼ì„œ ë‹¤ìŒ ì ‘ê·¼ì€ Gaussian process prior in the alternative model. they
embed the parametric null model in an infinite-dimensional exponential
family to form the alternative model ( ë¬´ìŠ¨ ëœ»ì´ì§€..)

The null model is defined by â„±â€„=â€„{*F*(â€…â‹…â€…â€…âˆ£â€…*Î¸*)â€„:â€„*Î¸*â€„âˆˆâ€„*Î©*} and a
random variable from this model is expressed as
*Y*â€„=â€„*F*<sup>â€…âˆ’â€…1</sup>(*U*â€…âˆ£â€…*Î¸*) where *U*â€„âˆ¼â€„ğ’°(0,â€†1) and *Î¸*â€„âˆˆâ€„*Î©*,
using the inverse probability integral transform.

The alternative model is an extended model defined by the
infinite-dimensional exponential family of distributions on \[0,â€†1\],
$\\mathcal{G}=\\{G(\\cdot \\mid \\psi): \\psi \\in \\S\\} .$ If a random
variable *Y* was from this extended model, then *Y* could be expressed
as *Y*â€„=â€„*F*<sup>â€…âˆ’â€…1</sup>(*U*â€…âˆ£â€…*Î¸*) where
*U*â€„âˆ¼â€„*G*(â€…â‹…â€…â€…âˆ£â€…*Ïˆ*),â€†*Î¸*â€„âˆˆâ€„*Î©* and *Ïˆ*â€„âˆˆâ€„*S*.

When *Ïˆ*â€„=â€„*Ïˆ*<sub>0</sub>,â€†*G*(â‹…âˆ£*Ïˆ*<sub>0</sub>)â€„=â€„ğ’°(0,â€†1) and hence
ğ’¢â€„=â€„â„±.

The probability densities associated with ğ’¢ can be written as

$$
g(u \\mid \\psi)=\\exp \\left(\\sum\_{j=1}^{\\infty} \\psi\_{j} \\phi\_{j}(u)-c(\\psi)\\right)
$$

where *Ïˆ*â€„=â€„(*Ïˆ*<sub>1</sub>,*Ïˆ*<sub>2</sub>,â€¦) are the polynomial
coefficients for the rescaled Legendre polyno- mials
*Ï•*â€„=â€„(*Ï•*<sub>1</sub>,*Ï•*<sub>2</sub>,â€¦) and *c*(*Ïˆ*) is the
normalizing constant.

The unknown parameters, *Î¸* and *Ïˆ* are taken to be independent such
that the prior distribution is given by *p*(*Î¸*,â€†*Ïˆ*)= *p*(*Î¸*)*p*(*Ïˆ*).

Take *p*(*Î¸*) to be any standard reference prior for *Î¸* and
$p(\\psi)=\\prod\_{j=1}^{\\infty} p\\left(\\psi\_{j} \\mid \\tau\\right) p(\\tau)$
such that each *Ïˆ*<sub>*j*</sub>â€„âˆ¼â€„*N*(0,*Ï„*<sup>2</sup>) and *Ï„*
follows a truncated standard normal distribution on \[0,â€†âˆ).

Under this construction,
$\\sum\_{j=1}^{\\infty} \\psi\_{j} \\phi\_{j}(u)$ is a Gaussian process.

MH in gibbsìƒ˜í”Œë§ì„ ì´ìš©í•˜ì—¬ ë² ì´ì¦ˆí™í„° ê³„ì‚°í•˜ë‹¤. ì´ ë°©ì‹ì€ ë‹¨ë³€ëŸ‰
ë°ì´í„°ì— ëŒ€í•´ì„œ ì—°ì†ì ì¸ nullëª¨ë¸ ì ìš©ê°€ëŠ¥í•˜ë‹¤.

ë˜ ë‹¤ë¥¸ ë°©ì‹ì€ í´ë´íŠ¸ë¦¬ í”„ë¡œì„¸ìŠ¤ì˜ í˜¼í•©ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì í•©ë„ ê²€ì •ì´ë‹¤.
í´ë´íŠ¸ë¦¬ëŠ” ë¹„ëª¨ìˆ˜ì ìœ¼ë¡œ ì—°ì†ì ì¸ ë°€ë„í•¨ìˆ˜ë¥¼ ëª¨ë¸ë§í•˜ê³  ë¬´ì •ë³´ì‚¬ì „ë¶„í¬ë¥¼
ì¤„ ìˆ˜ ìˆë‹¤.

ë””ë¦¬í´ë ˆí”„ë¡œì„¸ìŠ¤ë³´ë‹¤ ìš°ìˆ˜í•˜ì§€ë§Œ ì°¨ì›ì´ ì¦ê°€í•¨ì— ë”°ë¼ í™•ì¥ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ
ë‹¨ëŸ‰ ë°ì´í„°ë¡œ ì œí•œëœë‹¤.

ë˜ ë‹¤ë¥¸ ë°©ë²• by Tokdar and Martin (2013).

test for normality in any dimension using a non- subjective Dirichlet
process mixture of normals as the alternative model.

ì •ê·œì„± í…ŒìŠ¤íŠ¸ì—ë§Œ ì ìš©ë˜ëŠ” ì œí•œì´ ìˆë‹¤.

ì—¬ëŸ¬ê°€ì§€ ì í•©ë„ í…ŒìŠ¤íŠ¸ ë°©ë²•ì´ ìˆì§€ë§Œ ê° ë² ì´ì§€ì•ˆ ì ‘ê·¼ë²•ì—ëŠ” ê°ê°ì˜
ë‹¨ì ì´ ìˆë‹¤. ë‹¨ë³€ëŸ‰ ë°ì´í„° ì œì•ˆ, ì—°ì†nullëª¨ë¸ ì œí•œ, normal null ëª¨ë¸
ì œí•œ ë˜ ì£¼ë³€ìš°ë„ê³„ì‚°ì˜ ë³µì¡í•¨ ë“±ë“± ì—¬ëŸ¬ ë‹¨ì ì´ ì¡´ì¬í•œë‹¤.

ì—¬ê¸°ì„œ ì œì•ˆí•˜ëŠ” ë©”ì¸ ë°©ë²•ì€ ì ˆëŒ€ì  ì—°ì† null ëª¨ë¸ì— ëŒ€í•œ ë‹¤ë³€ëŸ‰ ì í•©ë„
ê²€ì •ì„ í•˜ê¸°ìœ„í•œ ê°„ë‹¨í•˜ê³  ì§ê´€ì ì¸ ë°©ì‹ì´ë‹¤.

1.2 Research Layout
-------------------

ì±•í„° 2ì—ì„œëŠ” examining the univariate *C**V**B**F*<sub>*K*</sub>
methodí•˜ê³  ë‹¤ë³€ëŸ‰ ë°ì´í„°ë¥¼ ê³ ë ¤í•˜ê¸°ì „ì— ê¸°ì´ˆë¥¼ ë‹¤ì§„ë‹¤.

ì±•í„° 3ì—ì„œëŠ” ë‹¤ë³€ëŸ‰ ì»¤ë„ ë°€ë„ì¶”ì •ì¹˜ë¥¼ ê°„ëµí•˜ê²Œ ì†Œê°œí•˜ê³ , bandwidth
matrixê°€ *C**V**B**F*<sub>*k*</sub>ì— ì ì¬ì ìœ¼ë¡œ ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì†Œê°œí•œë‹¤.

ì±•í„° 4ì—ì„œëŠ” 2ì¥ê³¼ 3ì¥ì„ ê²°í•©í•˜ì—¬ ë‹¤ë³€ëŸ‰ìœ¼ë¡œí™•ì¥í•œë‹¤. (ë©”ì¸ íŒŒíŠ¸)

ì±•í„° 5ì—ì„œëŠ” ëª¨ìˆ˜ì  cvbfë°©ë²• ì„¤ëª….

ì±•í„° 6ì€ 1-5ì¥ ë‚´ìš©ì„ í•©ì³ í–¥í›„ ì—°êµ¬ì— ëŒ€í•œ ì„¤ëª….

2. UNIVARIATE *C**V**B**F*<sub>*K*</sub> METHOD
===============================================

2.1 General Description
-----------------------

called the kernel cross-validated Bayes factor method because we need to
use data splitting in order to compute a kernel estimate.

ìˆ˜ë§ì€ ë¬´ì‘ìœ„ ë¶„í• ì— ëŒ€í•œ ê°ê°•ì˜ ë² ì´ì¦ˆíŒ©í„°ì˜ ê¸°í•˜í•™ì  í‰ê· ì„ ê²°ê³¼ê°’ìœ¼ë¡œ
í•œë‹¤.

2.2 Formal Methodology
----------------------

ì•Œë ¤ì§€ì§€ì•Šì€ ëª¨ìˆ˜ë°€ë„í•¨ìˆ˜ gì˜ ìƒ˜í”Œ:
**X**â€„=â€„(*X*<sub>1</sub>,*X*<sub>2</sub>,â€¦,*X*<sub>*n*</sub>)

Suppose we want to test that *g*â€„=â€„*f*(â€…â‹…â€…â€…âˆ£â€…*Î¸*), ì—¬ê¸°ì„œ f ëŠ” ëª¨ìˆ˜ë²¡í„°
ì„¸íƒ€ë¡œ ì¸ë±ì‹±ëœ íŠ¹ì • ë°€ë„ í•¨ìˆ˜ì´ë‹¤

ì—¬ê¸°ì„œ ê°€ì„¤ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
$$
\\begin{array}{l}
H\_{0}: \\mathbf{X}^{V} \\sim M\_{0}=\\{f(\\cdot \\mid \\theta): \\theta \\in \\Theta\\} \\\\
H\_{1}: \\mathbf{X}^{V} \\sim M\_{1}=\\left\\{\\hat{f}\\left(\\cdot \\mid \\mathbf{X}^{T}, h\\right): h&gt;0\\right\\} .
\\end{array}
$$
In the alternative model,
$$
\\hat{f}\\left(x \\mid \\mathbf{X}^{T}, h\\right)=\\frac{1}{m h} \\sum\_{i=1}^{m} K\_{1}\\left(\\frac{x-X\_{i}}{h}\\right)
$$
is the typical univariate kernel density estimator, *K*<sub>1</sub> ëŠ”
symmetric, unimodal, finite variance density functionìœ¼ë¡œ
ì·¨í•œë‹¤.(recommend using the Gaussian kernel function)

ë°ì´í„° ë¶„í• 

í›ˆë ¨ ë°ì´í„°:
**X**<sup>*T*</sup>â€„=â€„(*X*<sub>1</sub>,*X*<sub>2</sub>,â€¦,*X*<sub>*m*</sub>).

ê²€ì¦ ë°ì´í„°:
**X**<sup>*V*</sup>â€„=â€„(*X*<sub>*m*â€…+â€…1</sub>,*X*<sub>*m*â€…+â€…2</sub>,â€¦,*X*<sub>*n*</sub>).

í›ˆë ¨ë°ì´í„° ì‚¬ì´ì¦ˆì˜ ì ì ˆí•œ ì„¤ì •ì€ ë‚˜ì¤‘ì— ì„¤ëª…í•œë‹¤.

ëŒ€ë¦½ê°€ì„¤ ëª¨ë¸ì„ ì„ í˜¸í•˜ëŠ” The Bayes factor for a single random splitì€
ë‹¤ìŒê³¼ ê°™ë‹¤.( ì—¬ê¸°ì„œ mì€ train dataset size)
$$
\\mathrm{BF}\_{m}=\\frac{\\int\_{0}^{\\infty} \\prod\_{j=m+1}^{n} \\hat{f}\\left(X\_{j} \\mid \\mathbf{X}^{T}, h\\right) p(h) d h}{\\int\_{\\Theta} \\prod\_{j=m+1}^{n} f\\left(X\_{j} \\mid \\theta\\right) \\pi(\\theta) d \\theta}
$$
ë¬´ì‘ìœ„ ë¶„í• ì— ëŒ€í•œ ì˜ì¡´ì„±ì„ ì™„í™”í•˜ê¸° ìœ„í•´ì„œ ë¶„í• ì€ k(1~N)ë²ˆ ë°˜ë³µí•œë‹¤.

The resulting *C**V**B**F*<sub>*m*,â€†*N*</sub> value is the geometric
mean,
$$
\\mathrm{CVBF}\_{m, N}=\\left(\\prod\_{k=1}^{N} \\mathrm{BF}\_{m, k}\\right)^{1 / N}
$$

BFë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì‚¬ì „ë¶„í¬ê°€ ìš”êµ¬ëœë‹¤. *Ï€*(*Î¸*) and *p*(*h*).

Hart and Choi (2016) suggest taking a unit-information, reference (UIR)
prior for *Ï€*(*Î¸*).

ì´ UIR priorë¥¼ ì‚¬ìš©í•˜ë©´ ë‹¨ë³€ëŸ‰ ì •ê·œì„±ì„ ê²€ì •í•  ë•Œ locationê³¼ scaleì—
ë¶ˆë©´í•˜ëŠ” ë² ì´ì¦ˆí™í„°ê°€ ìƒì„±ëœë‹¤.

ì ì ˆí•œ *p*(*h*)ë¥¼ ì°¾ëŠ”ê²ƒì€ ì¡°ê¸ˆ ë” ë³µì¡í•˜ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì»¤ë„ë°€ë„ì¶”ì €ì—ì„œ
bandwidthëŠ” scale ëª¨ìˆ˜ì²˜ëŸ¼ ì‘ìš©í•˜ë¯€ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë¨¼ì € scale-invariant,
improper priorë¥¼ ê³ ë ¤í•œë‹¤. *p*(*h*)â€„âˆâ€„*h*<sup>â€…âˆ’â€…1</sup>

ë¶€ì ì ˆí•œ ì‚¬ì „ë¶„í¬ë¥¼ ê³ ë ¤í•˜ë©´ BFê°€ ì–´ë–¤ ìƒìˆ˜ì— ë¹„ë¡€í•˜ê²Œ ë˜ê¸° ë•Œë¬¸ì—
intrinsic Bayes factor (IBF) idea proposed by Berger and Perrichi
(1996)ì˜ ì•„ì´ë””ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì ì ˆí•œ priorë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤.

take the minimal sample size so that
$L\\left(X\_{1} \\mid X\_{2}, h\\right) p(h) \\propto h^{-2} K\_{1}\\left(\\frac{X\_{1}-X\_{2}}{h}\\right)$
produces a proper posterior distribution.

For the Gaussian kernel function the proper posterior distribution has a
closed form given by
$$
p(h \\mid \\beta)=\\frac{2 \\beta}{\\sqrt{\\pi} h^{2}} \\exp \\left(-\\frac{\\beta^{2}}{h^{2}}\\right),
$$

where *Î²*<sup>2</sup> is a robust estimate of
.5*E*\[(*X*<sub>1</sub>âˆ’*X*<sub>2</sub>)<sup>2</sup>\]â€„=â€„*Ïƒ*<sup>2</sup>
calculated from the validation data

ì‚¬ì „ ë¶„í¬ì™€ ë¬´ê´€í•˜ê²Œ ê·€ë¬´,ëŒ€ë¦½ê°€ì„¤ ëª¨ë‘ì—ì„œ BFëŠ” consistent at an
exponential rate

marginal likelihood under the alternative model must be computed
numerically. We prefer to use the Laplace approximation

*C**V**B**F*<sub>*K*</sub>ë¥¼ êµ¬í˜„í•˜ë ¤ë©´ í›ˆë ¨ì‚¬ì´ì¦ˆ mê³¼ ë¬´ì‘ìœ„ ë¶„í• ìˆ˜ Në§Œ
ê²°ì •í•˜ë©´ ë©ë‹ˆë‹¤.

authors typically set N = 100.

í›ˆë ¨ ì„¸íŠ¸ê°€ ê´€ì¸¡ì¹˜ë¥¼ ì¶©ë¶„íˆ ê·¼í•˜ê¸°ìœ„í•´ ì»¤ë„ì¶”ì •ì¹˜ì— ê´€ì¸¡ì¹˜mì´ ì¶©ë¶„íˆ
í•„ìš”í•˜ë¯€ë¡œ 0.05n &lt; m &lt; 0.5n

ê²€ì¦ë°ì´í„°ë³´ë‹¤ ì»¤ì§€ë©´ ì•ˆëœë‹¤.

ì´ ê³¼ì •ì€ nullëª¨ë¸ ì—ì„œ ë°˜ë³µì ìœ¼ë¡œ ìƒ˜í”Œë§ í•˜ì—¬ ì´ ê°’ì´ 0ì„ ì´ˆê³¼í•˜ëŠ”
ë¹ˆë„ë¥¼ ê°€ì§€ê³  ê²°ì •í•˜ê¸° ë•Œë¬¸ì— ë¹ˆë„ì£¼ì˜ì  ëŠë‚Œì´ ìˆë‹¤.

2.3 Real Data Example: Kevlar Strand Data
-----------------------------------------

It is hypothesized that the lifetimes
(*X*<sub>1</sub>,*X*<sub>2</sub>,â€¦,*X*<sub>100</sub>) constitute a
random sample from a log-normal distribution

if the transformed data *Y*<sub>*i*</sub>â€„=â€„logâ€†(*X*<sub>*i*</sub>)
follow a normal distribution.

Figure 2.1 contains a histogram of the log(lifetimes) with a kernel
density estimate (Gaus-sian kernel, *h*â€„=â€„.318 ) and a normal curve
(*Î¼Ì‚*â€„=â€„4.84,â€†*ÏƒÌ‚*â€„=â€„1.24) overlaid.

<figure>
<img src="fig2.jpg" style="width:50.0%" alt="log(time to failure)ë¶„í¬. ì»¤ë„ë°€ë„ì¶”ì •ì¹˜(ì‹¤ì„ ), ì¶”ì •ëœ ì •ê·œ ì»¤ë¸Œ(ì ì„ )" /><figcaption aria-hidden="true">log(time to failure)ë¶„í¬. ì»¤ë„ë°€ë„ì¶”ì •ì¹˜(ì‹¤ì„ ), ì¶”ì •ëœ ì •ê·œ ì»¤ë¸Œ(ì ì„ )</figcaption>
</figure>

íˆìŠ¤í† ê·¸ë¨ê³¼ ì»¤ë„ì¶”ì •ì¹˜ëŠ” ì •ê·œê³¡ì„ ê³¼ ë¹„êµí•˜ì—¬ left skewed ë˜ì–´ìˆê³ 
í”¼í¬ì ë„ ë†’ë‹¤ ë”°ë¼ì„œ ì •ê·œì„±ì´ ë¶€ì ì ˆí•˜ë‹¤.

Under the normal null model, we assume the data come from a
*N*(*Î¼*,â€†*Ïƒ*) distribution. Hart and Choi (2016) provide a
normal-inverse gamma UIR prior distribution of the form,
$$
\\pi(\\mu, \\sigma \\mid \\bar{Y}, \\gamma)=\\left(2 \\pi \\sigma^{2}\\right)^{-1 / 2} \\exp \\left\[-\\frac{1}{2 \\sigma^{2}}(\\mu-\\bar{Y})^{2}\\right\] \\frac{2 \\gamma}{\\sqrt{\\pi} \\sigma^{2}} \\exp \\left\[-\\frac{\\gamma^{2}}{\\sigma^{2}}\\right\]
$$
where $\\gamma=\\hat{\\sigma} / \\sqrt{2}$ for
$\\hat{\\sigma}^{2}=\\frac{1}{n-m} \\sum\_{j=m+1}^{n}\\left(Y\_{j}-\\bar{Y}\\right)^{2}$
and $\\bar{Y}=\\frac{1}{n-m} \\sum\_{j=m+1}^{n} Y\_{j} .$ Using the
prior in equation ( 2.4 ) and normal likelihood for validation data
**Y**<sup>*V*</sup>, the marginal likelihood for the null model is,
$$
m\\left(\\mathbf{Y}^{V} \\mid M\_{0}\\right)=\\Gamma\\left(\\frac{n-m+1}{2}\\right)(n-m+1)^{-(n-m+2) / 2} \\pi^{-(n-m+1) / 2} \\hat{\\sigma}^{-(n-m)}
$$

ëŒ€ë¦½ê°€ì„¤ ëª¨ë¸ì€ ê°€ìš°ì‹œì•ˆ ì»¤ë„ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ
$\\hat{f}\\left(x \\mid \\mathbf{X}^{T}, h\\right)=\\frac{1}{m h} \\sum\_{i=1}^{m} K\_{1}\\left(\\frac{x-X\_{i}}{h}\\right)$,ì„
ì´ìš©í•˜ì—¬ ì»¤ë„ë°€ë„ ì¶”ì •ì„ í•˜ê³  ì‚¬ì „ë¶„í¬ë¡œëŠ”
$p(h \\mid \\beta)=\\frac{2 \\beta}{\\sqrt{\\pi} h^{2}} \\exp \\left(-\\frac{\\beta^{2}}{h^{2}}\\right)$ì„
ì´ìš©í•œë‹¤.

ë”°ë¼ì„œ ë§ˆì§€ë„ ê°€ëŠ¥ë„ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
$$
\\begin{aligned}
m\\left(\\mathbf{Y}^{V} \\mid M\_{1}\\right)=\\int\_{0}^{\\infty} \\prod\_{j=m+1}^{n}\\left\[(2 \\pi)^{-1 / 2}(m h)^{-1} \\sum\_{i=1}^{m} \\exp \\left(-\\frac{\\left(Y\_{j}-Y\_{i}\\right)^{2}}{2 h^{2}}\\right)\\right\] \\\\
& \\times\\left\[\\frac{2 \\beta}{\\sqrt{\\pi} h^{2}} \\exp \\left(-\\frac{\\beta^{2}}{h^{2}}\\right)\\right\] d h
\\end{aligned}
$$
calibration schemeì„ ì‚¬ìš©í•˜ë©´ CVWE <sub>*m*,â€†*N*</sub>ê°€ ê³„ì‚°ë˜ê³  in
*m*â€„=â€„{5,â€†6,â€†â€¦,â€†49,â€†50} using *N*â€„=â€„1,â€†000 random splits.

ê³¡ì„ ì„ ê·¸ë¦¬ë©´(fig2.2) m=30ì—ì„œ 7.241ì—ì„œ cVWEê°€ maximumì´ ëœë‹¤. ë”°ë¼ì„œ
m=30ìœ¼ë¡œ ì„¤ì •í•˜ê³ , ë‹¤ìŒìœ¼ë¡œ ì¶”ì •ëœ null normalëª¨ë¸ì—ì„œ 500ê°œì˜ ìƒ˜í”Œë“¤ì„
ì¶”ì¶œí•˜ì—¬ *C**V**W**E*<sub>30,â€†100</sub> 500ë²ˆ ê³„ì‚°í•˜ê³  íˆìŠ¤í† ê·¸ë¨ì„
ê·¸ë¦°ë‹¤.(fig2.2)

<figure>
<img src="fig3.jpg" style="width:50.0%" alt="Left: CVWE for N=1000, m = 5~50, Right: \mathrm{CVWE}_{30,100} from ì¶”ì •ëœ nullëª¨ë¸ì—ì„œ ì¶”ì¶œí•œ 500ê°œì˜ ëœë¤ìƒ˜í”Œë“¤" /><figcaption aria-hidden="true">Left: CVWE for N=1000, m = 5~50, Right: <span class="math inline"><em>C</em><em>V</em><em>W</em><em>E</em><sub>30,â€†100</sub></span> from ì¶”ì •ëœ nullëª¨ë¸ì—ì„œ ì¶”ì¶œí•œ 500ê°œì˜ ëœë¤ìƒ˜í”Œë“¤</figcaption>
</figure>

All 500 CVWE30,100 values are less than 0 indicating that when m = 30

ë§Œì•½ ê´€ì¸¡ëœ ë°ì´í„°ê°€ ì‹¤ì œ ì •ê·œë¶„í¬ë¼ë©´ CVWEëŠ” ìŒìˆ˜ê°€ ëœë‹¤.

ê´€ì¸¡ëœ CVWE <sub>30,â€†100</sub> value from the Kevlar data of
7.241â€„&gt;â€„5 indicates that there is very strong evidence against the
normal model, which implies log-normality of the original times to
failure is not appropriate.

2.4 Conclusions
---------------

Based on the methodology in Section 2.2, we can see how the CVBFK method
should naturally extend to multivariate data.

3. MULTIVARIATE KERNEL DENSITY ESTIMATIOn
=========================================

ë‹¤ë³€ëŸ‰ ë°ì´í„°ì— ëŒ€í•œ ì»¤ë„CVBFë¥¼ ìˆ˜í–‰í•˜ë ¤ë©´ ë‹¤ë³€ëŸ‰ ì»¤ë„ë°€ë„ ì¶”ì •ì— ëŒ€í•´ì„œ
ì´í•´í•  í•„ìš”ê°€ ìˆë‹¤.

3.1 Definition
--------------

To estimate a *d* -dimensional multivariate density function
*f*<sub>*d*</sub> for observed data **Y**=
(*Y*<sub>1</sub>,*Y*<sub>2</sub>,â€¦,*Y*<sub>*n*</sub>) where each
*Y*<sub>*i*</sub>â€„âˆˆâ€„â„<sup>*d*</sup>, we can use the multivariate kernel
density estimate which has the following general form (Wand and Jones,
1995 ),
$$
\\hat{f}\_{d}(\\mathbf{y} \\mid \\mathbf{Y}, \\mathbf{H})=n^{-1}|\\mathbf{H}|^{-1 / 2} \\sum\_{i=1}^{n} K\_{d}\\left(\\mathbf{H}^{-1 / 2}\\left(\\mathbf{y}-\\mathbf{Y}\_{i}\\right)\\right) .
$$

scalar smoothing parameterì˜€ë˜ hê°€ bandwidth matrix, Hë¡œ ë°”ë€Œì—ˆë‹¤.
symmetric, positive definite matrices.(h&gt;0ì´ì—ˆë˜ ê²ƒê³¼ ìœ ì‚¬í•œ ì œí•œ)

*K*<sub>*d*</sub>ëŠ” d-ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ ì»¤ë„í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.(ë‹¤ë¥¸ê²ƒë„
ê°€ëŠ¥í•˜ì§€ë§Œ ì´ê²ƒì„ ì¶”ì²œ)
$$
K\_{d}(\\mathbf{t})=(2 \\pi)^{-d / 2} \\exp \\left(-\\mathbf{t}^{T} \\mathbf{t} / 2\\right)=\\prod\_{l=1}^{d} K\_{1}\\left(t\_{l}\\right)
$$

d-variate Gaussian kernelì€ ë‹¨ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ ì»¤ë„ì˜ ê³±ìœ¼ë¡œ ì‘ì„±ë  ìˆ˜ ìˆê³ 
ì´ê²ƒì€ ë‹¤ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ ì»¤ë„ CVBFì— ë‹¨ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆì»¤ë„ CVBFì˜ ê²°ê³¼ë¥¼
ì ìš©í•  ìˆ˜ ìˆìŒì„ ëœ»í•œë‹¤.

Bandwidth Matrix Classes
------------------------

ë‹¤ë³€ëŸ‰ ì»¤ë„ ë°€ë„ ì¶”ì •ì—ì„œ bandwidth í–‰ë ¬ì€ 3ê°€ì§€ì˜ í´ë˜ìŠ¤ì¤‘ì—ì„œ
ê³ ë ¤í•œë‹¤. \* Full (Unconstrained): The class of all symmetric, positive
definite bandwidth matrices with $\\frac{d(d+1)}{2}$ parameters, denoted
as â„±.

-   Diagonal: The class of all diagonal bandwidth matrices with *d*
    parameters, ğ’Ÿ=
    {**H**=diag(*h*<sub>1</sub><sup>2</sup>,*h*<sub>2</sub><sup>2</sup>,â‹¯,*h*<sub>*d*</sub><sup>2</sup>):*h*<sub>*l*</sub>&gt;0,*l*=1,â‹¯,*d*}

-   Scalar: The class of all diagonal matrices indexed by a single,
    scalar bandwidth,
    ğ’®â€„=â€„{**H**=*h*<sup>2</sup>**I**<sub>*d*</sub>:*h*&gt;0}
